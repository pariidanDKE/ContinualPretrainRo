{
  "_ro_winogrande": {
    "results": {
      "_ro_winogrande": {
        "alias": "_ro_winogrande",
        "acc,none": 0.0,
        "acc_stderr,none": "N/A"
      }
    },
    "group_subtasks": {
      "_ro_winogrande": []
    },
    "configs": {
      "_ro_winogrande": {
        "task": "_ro_winogrande",
        "dataset_path": "OpenLLM-Ro/ro_winogrande",
        "training_split": "train",
        "validation_split": "validation",
        "doc_to_text": "def doc_to_text(doc):\n    answer_to_num = {\"1\": 0, \"2\": 1}\n    return answer_to_num[doc[\"answer\"]]\n",
        "doc_to_target": "def doc_to_target(doc):\n    idx = doc[\"sentence\"].index(\"_\") + 1\n    return doc[\"sentence\"][idx:].strip()\n",
        "doc_to_choice": "def doc_to_choice(doc):\n    idx = doc[\"sentence\"].index(\"_\")\n    options = [doc[\"option1\"], doc[\"option2\"]]\n    return [doc[\"sentence\"][:idx] + opt for opt in options]\n",
        "description": "",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 5,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "multiple_choice",
        "repeats": 1,
        "should_decontaminate": true,
        "doc_to_decontamination_query": "sentence",
        "metadata": {
          "version": 1.0
        }
      }
    },
    "versions": {
      "_ro_winogrande": 1.0
    },
    "n-shot": {
      "_ro_winogrande": 5
    },
    "higher_is_better": {
      "_ro_winogrande": {
        "acc": true
      }
    },
    "n-samples": {
      "_ro_winogrande": {
        "original": 1267,
        "effective": 1
      }
    },
    "samples": {
      "_ro_winogrande": [
        {
          "doc_id": 0,
          "doc": {
            "sentence": "Sarah a fost un chirurg mult mai bun decât Maria așa că _ întotdeauna a primit cazurile mai ușoare.",
            "option2": "Maria",
            "option1": "Sarah",
            "answer": "2",
            "qID": "3FCO4VKOZ4BJQ6IFC0VAIBK4KTWE7U-2"
          },
          "target": "întotdeauna a primit cazurile mai ușoare.",
          "arguments": [
            [
              "<start_of_turn>user\nCafeaua amară a fost beată fericit de Logan, dar nu de William, deoarece Râul Logan nu a dormit cu o noapte înainte.\n\n„Abilitatea de a termina ceea ce începeți este ceea ce trebuie să îmbunătățiți”, a spus Michael lui Brian, Michael uneori trebuie să fie cel care preia controlul.\n\nEl a vrut să meargă camping RV, dar ea a vrut un hotel de lux, deoarece RV a fost atât de murdar.\n\nLindsey se bucura să mănânce multă mâncare cu carbohidrați, dar nu și Victoria, pentru că Lindsey era nesănătos.\n\nJustin își creștea părul mult în acest an, spre deosebire de Samuel pentru că Justin a vrut să-l doneze.<end_of_turn>\n<start_of_turn>model\nSarah a fost un chirurg mult mai bun decât Maria așa că Sarah",
              " întotdeauna a primit cazurile mai ușoare."
            ],
            [
              "<start_of_turn>user\nCafeaua amară a fost beată fericit de Logan, dar nu de William, deoarece Râul Logan nu a dormit cu o noapte înainte.\n\n„Abilitatea de a termina ceea ce începeți este ceea ce trebuie să îmbunătățiți”, a spus Michael lui Brian, Michael uneori trebuie să fie cel care preia controlul.\n\nEl a vrut să meargă camping RV, dar ea a vrut un hotel de lux, deoarece RV a fost atât de murdar.\n\nLindsey se bucura să mănânce multă mâncare cu carbohidrați, dar nu și Victoria, pentru că Lindsey era nesănătos.\n\nJustin își creștea părul mult în acest an, spre deosebire de Samuel pentru că Justin a vrut să-l doneze.<end_of_turn>\n<start_of_turn>model\nSarah a fost un chirurg mult mai bun decât Maria așa că Maria",
              " întotdeauna a primit cazurile mai ușoare."
            ]
          ],
          "resps": [
            [
              [
                -55.0,
                false
              ]
            ],
            [
              [
                -58.5,
                false
              ]
            ]
          ],
          "filtered_resps": [
            [
              -55.0,
              false
            ],
            [
              -58.5,
              false
            ]
          ],
          "doc_hash": "7d6f33b2e6b2bd8ad9a568353f4cd8bbcf321b1bbefc957df25cd501c1334b08",
          "prompt_hash": "a13d72e0bac7fa471a93adb0b79ae20970e08b4456f5e0e1987b4c991a1e1d83",
          "target_hash": "85068e521d5fd2cc2a8f1df4aa9c7d36041b3b9ebcebac58a9b8f032fe079555",
          "acc": 0.0
        }
      ]
    },
    "config": {
      "model": "hf",
      "model_args": "pretrained=google/gemma-3-1b-it",
      "model_num_parameters": 999885952,
      "model_dtype": "torch.bfloat16",
      "model_revision": "main",
      "model_sha": "dcc83ea841ab6100d6b47a070329e1ba4cf78752",
      "batch_size": 16,
      "batch_sizes": [],
      "device": "auto",
      "use_cache": null,
      "limit": 1,
      "bootstrap_iters": 100000,
      "gen_kwargs": null,
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    },
    "git_hash": "7b95d1c",
    "date": 1762625884.36128,
    "pretty_env_info": "PyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 15.6 (arm64)\nGCC version: Could not collect\nClang version: 16.0.0 (clang-1600.0.26.6)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.12.0 (main, Aug  2 2025, 13:07:06) [Clang 16.0.0 (clang-1600.0.26.6)] (64-bit runtime)\nPython platform: macOS-15.6-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M4\n\nVersions of relevant libraries:\n[pip3] numpy==2.3.3\n[pip3] torch==2.8.0\n[conda] Could not collect",
    "transformers_version": "4.56.2",
    "upper_git_hash": "7b95d1ca93f6e69c6d42332ae47f4bac9d3c2993",
    "tokenizer_pad_token": [
      "<pad>",
      "0"
    ],
    "tokenizer_eos_token": [
      "<eos>",
      "1"
    ],
    "tokenizer_bos_token": [
      "<bos>",
      "2"
    ],
    "eot_token_id": 1,
    "max_length": 32768
  }
}