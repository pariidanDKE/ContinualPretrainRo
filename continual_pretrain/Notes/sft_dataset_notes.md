Alpaca:

    - general-purpose broad instructions (like refactor code, find area of circle, asnwer to caeaar died)
    - 52k rows
    - generated using an LLM (text-davinci-003)
    - good for getting model to behave like generic helpful assistnat

Dolly: SELECTED
    - human-written
    - 15k rows
    - tries to help in instriction following from real-human phrasing
    - has categories ( Summarize, Generation, Classifiy, Chat, Extract, OpenQA) from InstructGPT

NoRobots: SELECTED
    - high quality human authored instriuctions ( travel advice, historical facts, 'health' advice etc)
    - 10k rows
    - has categories ( Summarize, Generation, Classifiy, Chat, Extract, OpenQA) from InstructGPT


- OpenAssistant Conversations: (maybe) SELECTED
    - human conversation corpus
    - 360k rows
    - assistant-style behaviour with real humans users
    - helps in aliging human conversational norms, politeness etc

- Camel Dataset: SELECTED
    - instruction following data generated by gpt-4
    - more school-questions oriented ( might help with some benchmarks), has domains like math, chemistry, biology, physics;
    - 110k rows

- Orca: SELECTED
    - inlcudes more CoT-style data, transaklted with systran
    - 360k rows

- UltaChat:
    - 700k rows
    - has multi-turn conversations
    



Instruct training run : 2B